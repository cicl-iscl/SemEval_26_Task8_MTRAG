{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRSR6CeU6TZD"
      },
      "outputs": [],
      "source": [
        "!pip install -U qwen\n",
        "!pip -q install faiss-cpu datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import faiss"
      ],
      "metadata": {
        "id": "1JmngU7cT4r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "REF_DIR = Path(\"/content/drive/MyDrive/dataset/reference.jsonl\")"
      ],
      "metadata": {
        "id": "zNJl-E4XT7pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLLECTION_TO_RESOURCES = {\n",
        "    \"clapnq\": {\n",
        "        \"corpus_path\": \"/content/drive/MyDrive/corpus/clapnq_corpus.jsonl\",\n",
        "        \"index_path\":  \"/content/drive/MyDrive/index/clapnq.faiss\",\n",
        "    },\n",
        "    \"cloud\": {\n",
        "        \"corpus_path\": \"/content/drive/MyDrive/corpus/cloud_.jsonl\",\n",
        "        \"index_path\":  \"/content/drive/MyDrive/index/cloud.faiss\",\n",
        "    },\n",
        "    \"fiqa\": {\n",
        "        \"corpus_path\": \"/content/drive/MyDrive/corpus/fiqa_.jsonl\",\n",
        "        \"index_path\":  \"/content/drive/MyDrive/index/fiqa.faiss\",\n",
        "    },\n",
        "    \"govt\": {\n",
        "        \"corpus_path\": \"/content/drive/MyDrive/corpus/govt_corpus.jsonl\",\n",
        "        \"index_path\":  \"/content/drive/MyDrive/index/mt_rag_govt.faiss\",\n",
        "    },\n",
        "}\n",
        "QUERIES_PATH = f\"{BASE_DIR}/history_selected_rewrite_queries/rewritten_last_turn_qwen3_30B.jsonl\""
      ],
      "metadata": {
        "id": "KlYR80fBZGv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus(obj):\n",
        "    coll = obj.get(\"Collection\") or obj.get(\"collection\")\n",
        "    if \"clapnq\" in coll:\n",
        "        return \"clapnq\"\n",
        "    elif \"cloud\" in coll:\n",
        "        return \"cloud\"\n",
        "    elif \"fiqa\" in coll:\n",
        "        return \"fiqa\"\n",
        "    elif \"govt\" in coll:\n",
        "        return \"govt\""
      ],
      "metadata": {
        "id": "kxHuUb5yUGac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE = {}  # corpus_name -> dict(index, ids, offsets, corpus_path)\n",
        "\n",
        "def load_corpus_cache(corpus_name):\n",
        "    if corpus_name in CACHE:\n",
        "        return CACHE[corpus_name]\n",
        "\n",
        "    corpus_path = COLLECTION_TO_RESOURCES[corpus_name][\"corpus_path\"]\n",
        "    index_path  = COLLECTION_TO_RESOURCES[corpus_name][\"index_path\"]\n",
        "\n",
        "    # 1) load faiss index once\n",
        "    index = faiss.read_index(index_path)\n",
        "\n",
        "    # 2) build (ids, offsets) without loading all texts\n",
        "    ids, offsets = [], []\n",
        "    offset = 0\n",
        "    with open(corpus_path, \"rb\") as f:          # binary mode for exact offsets\n",
        "        for line in f:\n",
        "            offsets.append(offset)\n",
        "            obj = json.loads(line.decode(\"utf-8\"))\n",
        "            ids.append(obj[\"_id\"])\n",
        "            offset += len(line)\n",
        "\n",
        "    CACHE[corpus_name] = {\n",
        "        \"index\": index,\n",
        "        \"ids\": ids,\n",
        "        \"offsets\": offsets,\n",
        "        \"corpus_path\": corpus_path,\n",
        "    }\n",
        "    return CACHE[corpus_name]\n",
        "\n",
        "def read_doc_by_idx(cache, i):\n",
        "    corpus_path = cache[\"corpus_path\"]\n",
        "    offset = cache[\"offsets\"][i]\n",
        "    with open(corpus_path, \"rb\") as f:\n",
        "        f.seek(offset)\n",
        "        line = f.readline()\n",
        "    obj = json.loads(line.decode(\"utf-8\"))\n",
        "    return obj[\"text\"]"
      ],
      "metadata": {
        "id": "tDTIUabDsGSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-4B\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RjSyTInvdYuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def embed_text(text, max_length=512):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    last_hidden = outputs.last_hidden_state\n",
        "    mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "\n",
        "    emb = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "    emb = emb.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "    # cosine normalize\n",
        "    emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "    return emb   # shape: (1, dim)"
      ],
      "metadata": {
        "id": "dSR8mD9Qe2Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(obj, question, top_k=5):\n",
        "    corpus = get_corpus(obj)\n",
        "    cache = load_corpus_cache(corpus)\n",
        "\n",
        "    q = embed_text(question)  # (1, dim) float32 normalized\n",
        "    scores, idxs = cache[\"index\"].search(q, top_k)\n",
        "\n",
        "    results = []\n",
        "    for score, i in zip(scores[0].tolist(), idxs[0].tolist()):\n",
        "        if i == -1:\n",
        "            continue\n",
        "        text = read_doc_by_idx(cache, i)\n",
        "        results.append({\n",
        "            \"doc_id\": cache[\"ids\"][i],\n",
        "            \"score\": float(score),\n",
        "            \"text\": text,\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "6VQ2_wAzfzyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "base_model = \"Qwen/Qwen3-14B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# adding LoRA adapter\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/rag_lora_adapter_qwen3_14B_try3\"\n",
        "\n",
        ")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "CY0Bc1rmiwGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(history, current, context):\n",
        "    contexts_sorted = sorted(context, key=lambda x: x[\"score\"], reverse=True)\n",
        "    formatted = []\n",
        "    for i, r in enumerate(contexts_sorted, start=1):\n",
        "        formatted.append(f\"[Document #{i}]\\n{r['text']}\\n\")\n",
        "    context_with_score = \"\\n\".join(formatted)\n",
        "    if history:\n",
        "      limited = []\n",
        "      if(len(history) > 2):\n",
        "        limited = history[-4:]\n",
        "      else:\n",
        "        limited = history[-2:]\n",
        "      history_text = \"\\n\".join(f\"{h[0]}: {h[1]}\" for h in limited)\n",
        "    else:\n",
        "        history_text = \"\"\n",
        "\n",
        "    instruct = \"\"\"\n",
        "                You are a RAG answer generator.\n",
        "\n",
        "                Use the provided reference documents and the conversation history as evidence.\n",
        "                Do not add information that is not supported by the reference.\n",
        "\n",
        "                If the reference fully answers the question:\n",
        "                - Provide a direct answer in 1â€“2 sentences.\n",
        "\n",
        "                If the reference provides only partial information:\n",
        "                - Sentence 1 must state that a specific detail needed to fully answer the question is missing.\n",
        "                  You may describe the missing detail in natural language (for example: \"whether\", \"how\", \"under what conditions\", or \"which case\").\n",
        "                - Sentence 2 must start with \"However,\" and include what the reference supports.\n",
        "\n",
        "                If the reference does not contain relevant information:\n",
        "                - Respond with exactly: \"I don't know.\"\n",
        "\n",
        "                If the question is conversational AND does not request factual information:\n",
        "                - Reply conversationally and briefly.\n",
        "                \"\"\".strip()\n",
        "    prompt = f\"\"\"\n",
        "              REFERENCE:\n",
        "              {context_with_score}\n",
        "\n",
        "              HISTORY:\n",
        "              {history_text}\n",
        "\n",
        "              QUESTION:\n",
        "              {current}\n",
        "\n",
        "              Answer:\n",
        "            \"\"\".strip()\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": instruct},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,enable_thinking=False,)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    end_id = tokenizer.convert_tokens_to_ids(\"END\") if \"END\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
        "\n",
        "    output_ids = model.generate(\n",
        "           **inputs,\n",
        "          max_new_tokens=256,\n",
        "          do_sample=False,\n",
        "          temperature=0.0,\n",
        "          eos_token_id=tokenizer.eos_token_id,\n",
        "      )\n",
        "\n",
        "\n",
        "    generated = output_ids[0]\n",
        "    answer_ids = generated[len(inputs[\"input_ids\"][0]):]\n",
        "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "    #fin_answer = json.loads(json_answer)\n",
        "    #answer = fin_answer[\"text\"]\n",
        "    return answer.strip()\n"
      ],
      "metadata": {
        "id": "-m97kPsBjBY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/dataset/rag_pipeline.jsonl\"\n",
        "with open(REF_DIR, \"r\") as fin, open(output_path, \"w\") as fout:\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "\n",
        "\n",
        "        history = [(cov[\"speaker\"],cov[\"text\"]) for cov in obj[\"input\"]][:-1]\n",
        "        current = [x[\"text\"] for x in obj[\"input\"]][-1]\n",
        "        context = retrieve(obj, current)\n",
        "\n",
        "        # generator\n",
        "        prediction = generate_answer(history, current, context)\n",
        "\n",
        "        prediction = \" \".join(prediction.split())\n",
        "\n",
        "        obj[\"predictions\"] = [{\"text\": prediction}]\n",
        "\n",
        "        fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "4CEGS06pjTpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}