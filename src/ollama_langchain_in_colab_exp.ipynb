{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAwFWcqctBaQ"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyv-EmpIpJIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe1b17c-faec-4c64-96ed-76c6ff8dd52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain langchain_community langchain_ollama langmem langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBn1qnqvtENb",
        "outputId": "12b54795-4512-443e-ab50-4055e0d9ad81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_PROJECT'] = \"ba_thesis_grad\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyqgM24RtaMG"
      },
      "source": [
        "# 2. Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icsJNXT5pP8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class Stage:\n",
        "  def __init__(self,\n",
        "               data_path:str,\n",
        "               batch:int=32):\n",
        "    self._data_path = data_path\n",
        "    self._batch = batch\n",
        "    self._df = pd.read_csv(self._data_path)\n",
        "    self._list_of_dicts = self._df.to_dict(orient=\"records\")\n",
        "    self._lists_of_lists_dicts = [self._list_of_dicts[i:i+self._batch] for i in range(0, len(self._list_of_dicts), self._batch)]\n",
        "    self.prompts = [\"You are an assistant who will classify the email. The category is Technical Support, Product Support, Customer Service, IT Support, Billing and Payments\"]\n",
        "    self.prompts_backup = list()\n",
        "    self.best_prompt = \"You are an assistant who will classify the email. The category is Technical Support, Product Support, Customer Service, IT Support, Billing and Payments\"\n",
        "    self.round = 1\n",
        "\n",
        "  def have_32_batches(self):\n",
        "    return len(self.lists_of_lists_dicts) > 0\n",
        "  def pop_32_batch(self):\n",
        "    if len(self._lists_of_lists_dicts) > 0:\n",
        "      return self._lists_of_lists_dicts.pop()\n",
        "    else:\n",
        "      raise Exception(\"No more 32 batches left\")\n",
        "  def pop_prompt(self):\n",
        "    if len(self.prompts) > 0:\n",
        "      return self.prompts.pop()\n",
        "    else:\n",
        "      return self.best_prompt\n",
        "  def update_prompts(self, best_prompt: str, prompts: list):\n",
        "    self.best_prompt = best_prompt\n",
        "    self.prompts = prompts.copy()\n",
        "    self.prompts_backup = prompts.copy()\n",
        "\n",
        "  def reset_prompts(self):\n",
        "    self.prompts = self.prompts_backup.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlY5E1gtQbI"
      },
      "outputs": [],
      "source": [
        "# train_path = \"/content/drive/MyDrive/ba_thesis/train_set.csv\"\n",
        "# test_path = \"/content/drive/MyDrive/ba_thesis/test_set.csv\"\n",
        "# stage = Stage(train_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13x5PbuyubLu"
      },
      "source": [
        "# 3. Ollama setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8coxUS43uha0"
      },
      "source": [
        "# curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfqqUR9Tud64",
        "outputId": "9bc34564-5840-4143-fc1d-b7107c1a8f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ollama version is 0.11.6\n",
            "NAME          ID              SIZE      MODIFIED       \n",
            "qwen3:0.6b    7df6b6e09427    522 MB    20 seconds ago    \n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# !ollama serve\n",
        "!ollama --version\n",
        "!ollama list\n",
        "# !ollama pull qwen3:0.6b\n",
        "!ollama pull qwen3:0.6b\n",
        "# !ollama pull llama3.2:1b\n",
        "# !ollama pull gemma3:1b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yof-XNS0ulq8"
      },
      "source": [
        "# 4. Helper Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvWdKTPduohT"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import csv\n",
        "import re\n",
        "\n",
        "def call_with_timeout(fn, timeout, max_retries=10, *args, **kwargs):\n",
        "    def wrapper(queue, *args, **kwargs):\n",
        "        try:\n",
        "            result = fn(*args, **kwargs)\n",
        "            queue.put(result)\n",
        "        except Exception as e:\n",
        "            queue.put(e)\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        queue = multiprocessing.Queue()\n",
        "        process = multiprocessing.Process(target=wrapper, args=(queue, *args), kwargs=kwargs)\n",
        "        process.start()\n",
        "        process.join(timeout)\n",
        "\n",
        "        if process.is_alive():\n",
        "            process.terminate()\n",
        "            process.join()\n",
        "            print(f\"Timeout after {timeout}s. Retrying... ({retries + 1}/{max_retries})\")\n",
        "            retries += 1\n",
        "        else:\n",
        "            result = queue.get()\n",
        "            if isinstance(result, Exception):\n",
        "                print(f\"Error on attempt {retries + 1}: {result}\")\n",
        "                retries += 1\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "    raise TimeoutError(f\"Function exceeded timeout or failed after {max_retries} retries.\")\n",
        "\n",
        "def answers2trajectories(answers: list):\n",
        "  trajectories = []\n",
        "  for item in answers:\n",
        "    trajectories.append(([SystemMessage(content=item[\"prompt\"]), HumanMessage(content=str(item[\"subject\"])+item['body']), AIMessage(content=item[\"full_llm_answer\"])], item[\"correctness\"]))\n",
        "  return trajectories\n",
        "\n",
        "def append_samples_csv(new_samples, file_path):\n",
        "    # Assume new_samples is a list of dicts\n",
        "    file_exists = os.path.isfile(file_path)\n",
        "    fieldnames = new_samples[0].keys() if new_samples else []\n",
        "\n",
        "    with open(file_path, 'a', encoding='utf-8', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        for sample in new_samples:\n",
        "            writer.writerow(sample)\n",
        "\n",
        "def extract_prompts(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Extracts all content inside <prompt>...</prompt> tags from the input string.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input string containing one or more <prompt>...</prompt> blocks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of strings extracted from each <prompt>...</prompt> block.\n",
        "    \"\"\"\n",
        "    pattern = r\"<prompt>(.*?)</prompt>\"\n",
        "    return re.findall(pattern, text, re.DOTALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjsBU8t5wjJ-"
      },
      "source": [
        "## 4-2. Helper method: LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVi03ZCowlz0"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "class ResponseFormat(BaseModel):\n",
        "    reason: str = Field(description=\"The reason why you give the answer\")\n",
        "    answer: Literal[\"Technical Support\", \"Product Support\", \"Customer Service\", \"IT Support\", \"Billing and Payments\"] = Field(description=\"The Category that this mail belongs to\")\n",
        "\n",
        "def answer(subject:str, body:str, prompt: str, local_llm:str=\"qwen3:0.6b\"):\n",
        "  llm = ChatOllama(model=local_llm, temperature=0.6)\n",
        "  llm = llm.with_structured_output(ResponseFormat)\n",
        "  query = 'subject: ' + str(subject) + '\\nbody: ' + str(body)\n",
        "  output = llm.invoke([SystemMessage(content=prompt), HumanMessage(content=query)])\n",
        "  return output.answer, output.reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFZN4WpfeDj"
      },
      "outputs": [],
      "source": [
        "from langmem import create_prompt_optimizer\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langmem import Prompt\n",
        "\n",
        "def optimize(trajectories: list):\n",
        "    model = init_chat_model(\"gpt-4.1\", temperature=0.6)\n",
        "    prompt_optimizer = create_prompt_optimizer(model, kind=\"gradient\",\n",
        "                                        config={\"max_reflection_steps\": 1, \"min_reflection_steps\": 0})\n",
        "    prompting_instruction = \"You are an assistant who will classify the email. The category is Technical Support, Product Support, Customer Service, IT Support, Billing and Payments\"\n",
        "\n",
        "    #trajectories = [(conversation, feedback)]\n",
        "    #trajectories = [(conversation, None)]\n",
        "    # trajectories = [(conversation_1, feedback_1), (conversation_2, feedback_2)]\n",
        "    better_prompt = prompt_optimizer.invoke({\"trajectories\": trajectories, \"prompt\": prompting_instruction})\n",
        "\n",
        "    return better_prompt\n",
        "\n",
        "def optimize_phraseevo(trajectories: list):\n",
        "    model = init_chat_model(\"gpt-4.1\", temperature=0.6)\n",
        "    prompt = Prompt(name=\"generate_5_prompts\", prompt=\"You are an assistant who will classify the email. The category is Technical Support, Product Support, Customer Service, IT Support, Billing and Payments\",prompting_instruction=\"PLEASE GENERATE FIVE PROMPTS TO COVER DIFFERENT ASPECT AND PUT EACH PROMPT IN <prompt></prompt>.\")\n",
        "    prompt_optimizer = create_prompt_optimizer(model, kind=\"gradient\",\n",
        "                                        config={\"max_reflection_steps\": 1, \"min_reflection_steps\": 0})\n",
        "\n",
        "    better_prompt = prompt_optimizer.invoke({\"trajectories\": trajectories, \"prompt\": prompt})\n",
        "    # better_prompts = extract_prompts(better_prompt)\n",
        "    # return better_prompts\n",
        "\n",
        "    return better_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8geax8wvMm-"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def generate_multiple_prompts(trajectories: list, num_prompts: int = 5):\n",
        "    with ThreadPoolExecutor(max_workers=num_prompts) as executor:\n",
        "        futures = [executor.submit(optimize, trajectories) for _ in range(num_prompts)]\n",
        "        results = [future.result() for future in futures]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puHsLeQGu01M"
      },
      "source": [
        "# 5. Normal Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5S2UMGPu0Mr"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/ba_thesis/train_set.csv\"\n",
        "test_path = \"/content/drive/MyDrive/ba_thesis/test_set.csv\"\n",
        "log_path = \"/content/drive/MyDrive/ba_thesis/results/qwen3_0.6b/validation/cot_24-08.log\"\n",
        "output_filepath = \"/content/drive/MyDrive/ba_thesis/results/qwen3_0.6b/validation/cot_24-08.csv\"\n",
        "stage = Stage(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crbYQlVYu_Km",
        "outputId": "4c8577be-b53c-423d-9aae-e89f65b565c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-24 16:50:14,011 [CRITICAL] Processed 100 samples\n",
            "2025-08-24 16:51:45,776 [CRITICAL] Processed 200 samples\n",
            "2025-08-24 16:53:15,711 [CRITICAL] Processed 300 samples\n",
            "2025-08-24 16:54:49,869 [CRITICAL] Processed 400 samples\n",
            "2025-08-24 16:56:22,169 [CRITICAL] Processed 500 samples\n",
            "2025-08-24 16:57:54,026 [CRITICAL] Processed 600 samples\n",
            "2025-08-24 16:59:24,612 [CRITICAL] Processed 700 samples\n",
            "2025-08-24 17:00:55,350 [CRITICAL] Processed 800 samples\n",
            "2025-08-24 17:02:28,522 [CRITICAL] Processed 900 samples\n",
            "2025-08-24 17:04:00,473 [CRITICAL] Processed 1000 samples\n",
            "2025-08-24 17:05:34,402 [CRITICAL] Processed 1100 samples\n",
            "2025-08-24 17:07:06,011 [CRITICAL] Processed 1200 samples\n",
            "2025-08-24 17:08:37,786 [CRITICAL] Processed 1300 samples\n",
            "2025-08-24 17:10:11,060 [CRITICAL] Processed 1400 samples\n",
            "2025-08-24 17:11:44,060 [CRITICAL] Processed 1500 samples\n",
            "2025-08-24 17:13:15,808 [CRITICAL] Processed 1600 samples\n",
            "2025-08-24 17:14:47,821 [CRITICAL] Processed 1700 samples\n",
            "2025-08-24 17:16:19,213 [CRITICAL] Processed 1800 samples\n",
            "2025-08-24 17:17:53,768 [CRITICAL] Processed 1900 samples\n",
            "2025-08-24 17:19:27,625 [CRITICAL] Processed 2000 samples\n",
            "2025-08-24 17:20:59,845 [CRITICAL] Processed 2100 samples\n",
            "2025-08-24 17:22:32,650 [CRITICAL] Processed 2200 samples\n",
            "2025-08-24 17:24:05,040 [CRITICAL] Processed 2300 samples\n",
            "2025-08-24 17:25:35,593 [CRITICAL] Processed 2400 samples\n",
            "2025-08-24 17:27:05,355 [CRITICAL] Processed 2500 samples\n",
            "2025-08-24 17:28:37,418 [CRITICAL] Processed 2600 samples\n",
            "2025-08-24 17:30:11,100 [CRITICAL] Processed 2700 samples\n",
            "2025-08-24 17:31:43,985 [CRITICAL] Processed 2800 samples\n",
            "2025-08-24 17:33:16,723 [CRITICAL] Processed 2900 samples\n",
            "2025-08-24 17:34:48,912 [CRITICAL] Processed 3000 samples\n",
            "2025-08-24 17:36:19,584 [CRITICAL] Processed 3100 samples\n",
            "2025-08-24 17:37:53,177 [CRITICAL] Processed 3200 samples\n",
            "2025-08-24 17:39:26,262 [CRITICAL] Processed 3300 samples\n",
            "2025-08-24 17:41:00,025 [CRITICAL] Processed 3400 samples\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "No more 32 batches left",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-29981304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail_classification_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_32_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mdatalist_1x32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_32_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Batch: setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-247160641.py\u001b[0m in \u001b[0;36mpop_32_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists_of_lists_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No more 32 batches left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpop_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: No more 32 batches left"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[logging.FileHandler(log_path), logging.StreamHandler()], force=True)\n",
        "\n",
        "debug_counts = 0\n",
        "\n",
        "email_classification_prompt = \"\"\"You are an assistant who will classify the email. The category is Technical Support, Product Support, Customer Service, IT Support, Billing and Payments.\n",
        "Let's think step by step:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# prompt = stage.pop_prompt()\n",
        "prompt = email_classification_prompt\n",
        "while stage.have_32_batches:\n",
        "  datalist_1x32 = stage.pop_32_batch()\n",
        "\n",
        "  # Batch: setup\n",
        "  completed_samples = list()\n",
        "  # Batch operation\n",
        "  for sample in datalist_1x32:\n",
        "    #answer\n",
        "    answer_category, answer_reason = call_with_timeout(\n",
        "    answer,\n",
        "    timeout=5,\n",
        "    max_retries=100,\n",
        "    subject=sample['subject'],\n",
        "    body=sample['body'],\n",
        "    prompt=prompt)\n",
        "    #validation\n",
        "    sample['llm_answer'] = answer_category\n",
        "    sample['full_llm_answer'] = \"answer: \" + answer_category + \"\\nreason: \" + answer_reason\n",
        "    sample['prompt'] = prompt\n",
        "    sample['round'] = stage.round\n",
        "    sample['timestamp'] = datetime.now().isoformat()\n",
        "    sample[\"correctness\"] = (sample['queue'].lower() == sample['llm_answer'].lower())\n",
        "    completed_samples.append(sample)\n",
        "    debug_counts += 1\n",
        "    if debug_counts % 100 == 0:\n",
        "      logging.critical(f\"Processed {debug_counts} samples\")\n",
        "  # === Append to a csv file & Update Round===\n",
        "  append_samples_csv(completed_samples, output_filepath)\n",
        "\n",
        "logging.critical(\"ALL SAMPLES PROCESSED\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhF-7kfZNY8x"
      },
      "source": [
        "# 6. Text Gradient: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUvyjHIskpg0"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/ba_thesis/train_set.csv\"\n",
        "test_path = \"/content/drive/MyDrive/ba_thesis/test_set.csv\"\n",
        "log_path = \"/content/drive/MyDrive/ba_thesis/results/qwen3_8b/grad_17-08.log\"\n",
        "output_filepath = \"/content/drive/MyDrive/ba_thesis/results/qwen3_8b/grad_17-08.csv\"\n",
        "output_beamsearch_filepath = \"/content/drive/MyDrive/ba_thesis/results/qwen3_8b/grad_beam_17-08.csv\"\n",
        "stage = Stage(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WddyKOb9NXn7",
        "outputId": "63ae060c-5af2-46d4-9379-29b8cb81ad82"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-17 10:25:25,597 [CRITICAL] Processed 100 samples\n",
            "2025-08-17 10:29:20,820 [CRITICAL] Processed 200 samples\n",
            "2025-08-17 10:33:05,113 [CRITICAL] Processed 300 samples\n",
            "2025-08-17 11:08:24,359 [CRITICAL] Best Score: 0.3908683707596751\n",
            "2025-08-17 11:12:28,274 [CRITICAL] Processed 400 samples\n",
            "2025-08-17 11:51:55,590 [CRITICAL] Best Score: 0.3816307696277689\n",
            "2025-08-17 11:53:31,268 [CRITICAL] Processed 500 samples\n",
            "2025-08-17 11:57:55,114 [CRITICAL] Processed 600 samples\n",
            "2025-08-17 12:34:29,976 [CRITICAL] Best Score: 0.4635980890294954\n",
            "2025-08-17 12:37:45,722 [CRITICAL] Processed 700 samples\n",
            "2025-08-17 13:15:43,647 [CRITICAL] Best Score: 0.3869305120167189\n",
            "2025-08-17 13:16:25,115 [CRITICAL] Processed 800 samples\n",
            "2025-08-17 13:20:49,012 [CRITICAL] Processed 900 samples\n",
            "2025-08-17 13:58:39,835 [CRITICAL] Best Score: 0.3077047805474015\n",
            "2025-08-17 14:01:43,174 [CRITICAL] Processed 1000 samples\n",
            "2025-08-17 14:06:10,965 [CRITICAL] Processed 1100 samples\n",
            "2025-08-17 14:42:31,762 [CRITICAL] Best Score: 0.41514627715297864\n",
            "2025-08-17 14:47:04,516 [CRITICAL] Processed 1200 samples\n",
            "2025-08-17 15:26:43,483 [CRITICAL] Best Score: 0.38977162933684667\n",
            "2025-08-17 15:28:08,496 [CRITICAL] Processed 1300 samples\n",
            "2025-08-17 15:32:55,358 [CRITICAL] Processed 1400 samples\n",
            "2025-08-17 16:10:27,864 [CRITICAL] Best Score: 0.3647413115443806\n",
            "2025-08-17 16:13:32,892 [CRITICAL] Processed 1500 samples\n",
            "2025-08-17 16:52:24,864 [CRITICAL] Best Score: 0.4203582382529751\n",
            "2025-08-17 16:53:05,628 [CRITICAL] Processed 1600 samples\n",
            "2025-08-17 16:57:08,018 [CRITICAL] Processed 1700 samples\n",
            "2025-08-17 17:33:30,860 [CRITICAL] Best Score: 0.37825311942959\n",
            "2025-08-17 17:35:19,293 [CRITICAL] Processed 1800 samples\n",
            "2025-08-17 17:39:30,899 [CRITICAL] Processed 1900 samples\n",
            "2025-08-17 18:14:08,940 [CRITICAL] Best Score: 0.4000078492935636\n",
            "2025-08-17 18:18:01,346 [CRITICAL] Processed 2000 samples\n",
            "2025-08-17 18:53:19,261 [CRITICAL] Best Score: 0.40245974235104665\n",
            "2025-08-17 18:54:43,526 [CRITICAL] Processed 2100 samples\n",
            "2025-08-17 18:58:44,964 [CRITICAL] Processed 2200 samples\n",
            "2025-08-17 19:33:57,527 [CRITICAL] Best Score: 0.3810897305782216\n",
            "2025-08-17 19:37:03,328 [CRITICAL] Processed 2300 samples\n",
            "2025-08-17 20:15:04,503 [CRITICAL] Best Score: 0.33609719609719607\n",
            "2025-08-17 20:15:42,648 [CRITICAL] Processed 2400 samples\n",
            "2025-08-17 20:19:39,481 [CRITICAL] Processed 2500 samples\n",
            "2025-08-17 20:55:57,257 [CRITICAL] Best Score: 0.3833839851904368\n",
            "2025-08-17 20:58:12,323 [CRITICAL] Processed 2600 samples\n",
            "2025-08-17 21:02:11,632 [CRITICAL] Processed 2700 samples\n",
            "2025-08-17 21:36:54,063 [CRITICAL] Best Score: 0.39355102040816325\n",
            "2025-08-17 21:40:48,705 [CRITICAL] Processed 2800 samples\n",
            "2025-08-17 22:15:47,573 [CRITICAL] Best Score: 0.42425722425722423\n",
            "2025-08-17 22:17:19,237 [CRITICAL] Processed 2900 samples\n",
            "2025-08-17 22:20:50,453 [CRITICAL] Processed 3000 samples\n",
            "2025-08-17 22:55:25,924 [CRITICAL] Best Score: 0.36472727272727273\n",
            "2025-08-17 22:58:23,746 [CRITICAL] Processed 3100 samples\n",
            "2025-08-17 23:34:00,881 [CRITICAL] Best Score: 0.4403652340496419\n",
            "2025-08-17 23:34:42,805 [CRITICAL] Processed 3200 samples\n",
            "2025-08-17 23:38:38,456 [CRITICAL] Processed 3300 samples\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timeout after 10s. Retrying... (1/100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-18 00:18:35,753 [CRITICAL] Best Score: 0.4432072990300838\n",
            "2025-08-18 00:20:40,146 [CRITICAL] Processed 3400 samples\n",
            "2025-08-18 00:25:29,443 [CRITICAL] Processed 3500 samples\n",
            "2025-08-18 01:01:25,161 [CRITICAL] Best Score: 0.3495359573934921\n",
            "2025-08-18 01:05:49,924 [CRITICAL] Processed 3600 samples\n",
            "2025-08-18 01:44:12,452 [CRITICAL] Best Score: 0.4377335978379621\n",
            "2025-08-18 01:45:54,375 [CRITICAL] Processed 3700 samples\n",
            "2025-08-18 01:50:10,137 [CRITICAL] Processed 3800 samples\n",
            "2025-08-18 02:24:55,225 [CRITICAL] Best Score: 0.35437181409295354\n",
            "2025-08-18 02:28:01,076 [CRITICAL] Processed 3900 samples\n",
            "2025-08-18 03:06:09,364 [CRITICAL] Best Score: 0.3793815874017878\n",
            "2025-08-18 03:06:48,351 [CRITICAL] Processed 4000 samples\n",
            "2025-08-18 03:10:52,269 [CRITICAL] Processed 4100 samples\n",
            "2025-08-18 03:47:19,408 [CRITICAL] Best Score: 0.38981541334482517\n",
            "2025-08-18 03:49:53,139 [CRITICAL] Processed 4200 samples\n",
            "2025-08-18 03:53:35,471 [CRITICAL] Processed 4300 samples\n",
            "2025-08-18 04:29:48,984 [CRITICAL] Best Score: 0.4351131131375305\n",
            "2025-08-18 04:34:06,066 [CRITICAL] Processed 4400 samples\n",
            "2025-08-18 05:12:42,607 [CRITICAL] Best Score: 0.41588608825244605\n",
            "2025-08-18 05:14:19,979 [CRITICAL] Processed 4500 samples\n",
            "2025-08-18 05:18:54,324 [CRITICAL] Processed 4600 samples\n",
            "2025-08-18 05:54:50,697 [CRITICAL] Best Score: 0.39643637782982044\n",
            "2025-08-18 05:57:31,155 [CRITICAL] Processed 4700 samples\n",
            "2025-08-18 06:34:35,279 [CRITICAL] Best Score: 0.37525539893960946\n",
            "2025-08-18 06:35:16,274 [CRITICAL] Processed 4800 samples\n",
            "2025-08-18 06:39:44,366 [CRITICAL] Processed 4900 samples\n",
            "2025-08-18 07:15:18,433 [CRITICAL] Best Score: 0.4227247502774695\n",
            "2025-08-18 07:17:11,553 [CRITICAL] Processed 5000 samples\n",
            "2025-08-18 07:21:05,040 [CRITICAL] Processed 5100 samples\n",
            "2025-08-18 07:54:45,125 [CRITICAL] Best Score: 0.40639299397920087\n",
            "2025-08-18 07:58:58,321 [CRITICAL] Processed 5200 samples\n",
            "2025-08-18 08:34:27,105 [CRITICAL] Best Score: 0.4262991910050733\n",
            "2025-08-18 08:36:05,427 [CRITICAL] Processed 5300 samples\n",
            "2025-08-18 08:40:18,202 [CRITICAL] Processed 5400 samples\n",
            "2025-08-18 09:14:21,760 [CRITICAL] Best Score: 0.48261550848507373\n",
            "2025-08-18 09:17:38,369 [CRITICAL] Processed 5500 samples\n",
            "2025-08-18 09:54:52,426 [CRITICAL] Best Score: 0.3430307136404697\n",
            "2025-08-18 09:55:40,354 [CRITICAL] Processed 5600 samples\n",
            "2025-08-18 09:59:55,705 [CRITICAL] Processed 5700 samples\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[logging.FileHandler(log_path), logging.StreamHandler()], force=True)\n",
        "\n",
        "debug_counts = 0\n",
        "while stage.have_32_batches:\n",
        "  datalist_1x32 = stage.pop_32_batch()\n",
        "  prompt = stage.pop_prompt()\n",
        "\n",
        "  # Batch: setup\n",
        "  completed_samples = list()\n",
        "  # Batch operation\n",
        "  for sample in datalist_1x32:\n",
        "    #answer\n",
        "    answer_category, answer_reason = call_with_timeout(\n",
        "    answer,\n",
        "    timeout=10,\n",
        "    max_retries=100,\n",
        "    subject=sample['subject'],\n",
        "    body=sample['body'],\n",
        "    prompt=prompt)\n",
        "    #validation\n",
        "    sample['llm_answer'] = answer_category\n",
        "    sample['full_llm_answer'] = \"answer: \" + answer_category + \"\\nreason: \" + answer_reason\n",
        "    sample['prompt'] = prompt\n",
        "    sample['round'] = stage.round\n",
        "    sample['timestamp'] = datetime.now().isoformat()\n",
        "    sample[\"correctness\"] = (sample['queue'].lower() == sample['llm_answer'].lower())\n",
        "    completed_samples.append(sample)\n",
        "    debug_counts += 1\n",
        "    if debug_counts % 100 == 0:\n",
        "      logging.critical(f\"Processed {debug_counts} samples\")\n",
        "  # === Append to a csv file & Update Round===\n",
        "  append_samples_csv(completed_samples, output_filepath)\n",
        "  stage.round += 1\n",
        "  # === Update Prompt ===\n",
        "  trajectories = answers2trajectories(completed_samples)\n",
        "  better_prompt = optimize(trajectories)\n",
        "  stage.prompts_backup.append(better_prompt)\n",
        "\n",
        "  # === Beam Search ===\n",
        "  if len(stage.prompts_backup) >= 10:\n",
        "    prompts_scores = list()\n",
        "    datalist_1x32 = stage.pop_32_batch()\n",
        "    datalist_1x32.extend(stage.pop_32_batch())\n",
        "    datalist_1x32.extend(stage.pop_32_batch())\n",
        "    while len(stage.prompts_backup) > 0:\n",
        "      test_completed_samples = list()\n",
        "      prompt = stage.prompts_backup.pop()\n",
        "      for sample in datalist_1x32:\n",
        "        answer_category, answer_reason = call_with_timeout(\n",
        "        answer,\n",
        "        timeout=10,\n",
        "        max_retries=100,\n",
        "        subject=sample['subject'],\n",
        "        body=sample['body'],\n",
        "        prompt=prompt)\n",
        "        #validation\n",
        "        sample['llm_answer'] = answer_category\n",
        "        sample['full_llm_answer'] = \"answer: \" + answer_category + \"\\nreason: \" + answer_reason\n",
        "        sample['prompt'] = prompt\n",
        "        sample['round'] = stage.round\n",
        "        sample['timestamp'] = datetime.now().isoformat()\n",
        "        sample[\"correctness\"] = (sample['queue'].lower() == sample['llm_answer'].lower())\n",
        "        test_completed_samples.append(sample)\n",
        "      append_samples_csv(test_completed_samples, output_beamsearch_filepath)\n",
        "      #f1 score\n",
        "      df_test_completed_samples = pd.DataFrame(test_completed_samples)\n",
        "      f1_score = classification_report(df_test_completed_samples['queue'], df_test_completed_samples['llm_answer'], output_dict=True, zero_division=0)[\"macro avg\"][\"f1-score\"]\n",
        "      prompts_scores.append((prompt, f1_score))\n",
        "\n",
        "    top_5_prompts_scores = [(prompt, score) for prompt, score in sorted(prompts_scores, key=lambda x: x[1], reverse=True)[:5]]\n",
        "    best_prompt_score = None, 0\n",
        "    for prompt, score in top_5_prompts_scores:\n",
        "      if score > best_prompt_score[1]:\n",
        "        best_prompt_score = (prompt, score)\n",
        "    # logging.critical(f\"Best Prompt: {best_prompt_score[0]}\")\n",
        "    logging.critical(f\"Best Score: {best_prompt_score[1]}\")\n",
        "    stage.update_prompts(best_prompt_score[0], [prompt for prompt, score in top_5_prompts_scores])\n",
        "\n",
        "logging.critical(\"ALL SAMPLES PROCESSED\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tvtKZglykhU"
      },
      "source": [
        "# 7. Phrase Evo: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdCDZBZT0hi4"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/ba_thesis/train_set.csv\"\n",
        "test_path = \"/content/drive/MyDrive/ba_thesis/test_set.csv\"\n",
        "log_path = \"/content/drive/MyDrive/ba_thesis/results/qwen3_8b/phrase_16-08.log\"\n",
        "output_beamsearch_filepath = \"/content/drive/MyDrive/ba_thesis/results/qwen3_8b/phrase_beam_16-08.csv\"\n",
        "stage = Stage(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l0wAH6UaykCf",
        "outputId": "91b1f55c-2f2e-453c-a0f0-d41adf985512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timeout after 5s. Retrying... (1/100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process Process-409:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-2094784163.py\", line 8, in wrapper\n",
            "    result = fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-232796065.py\", line 15, in answer\n",
            "    output = llm.invoke([SystemMessage(content=prompt), HumanMessage(content=query)])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2437353313.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatalist_1x32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m#answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       answer_category, answer_reason = call_with_timeout(\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2094784163.py\u001b[0m in \u001b[0;36mcall_with_timeout\u001b[0;34m(fn, timeout, max_retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\", line 5441, in invoke\n",
            "    return self.bound.invoke(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 383, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1006, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 825, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1072, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_ollama/chat_models.py\", line 819, in _generate\n",
            "    final_chunk = self._chat_stream_with_aggregation(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_ollama/chat_models.py\", line 754, in _chat_stream_with_aggregation\n",
            "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_ollama/chat_models.py\", line 841, in _iterate_over_stream\n",
            "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_ollama/chat_models.py\", line 740, in _create_chat_stream\n",
            "    yield from self._client.chat(**chat_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ollama/_client.py\", line 172, in inner\n",
            "    for line in r.iter_lines():\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 929, in iter_lines\n",
            "    for text in self.iter_text():\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[logging.FileHandler(log_path), logging.StreamHandler()], force=True)\n",
        "\n",
        "debug_counts = 0\n",
        "stage.prompts = [\"Read the email and determine which of the following best describes the user's issue: Technical Support, Product Support, Customer Service, IT Support, or Billing and Payments.\",\n",
        "          \"Classify this customer email into one of the following categories to ensure it is routed to the appropriate team: Technical Support, Product Support, Customer Service, IT Support, Billing and Payments.\",\n",
        "                 \"Analyze the content of this email and assign it to the most relevant support category: Technical Support, Product Support, Customer Service, IT Support, or Billing and Payments.\",\n",
        "                 \"Based on the customer's intent expressed in the email, classify it into one of these categories: Technical Support, Product Support, Customer Service, IT Support, Billing and Payments.\",\n",
        "                 \"Review the following email and classify the type of service request it contains using these categories: Technical Support, Product Support, Customer Service, IT Support, Billing and Payments.\"]\n",
        "while stage.have_32_batches:\n",
        "  datalist_1x32 = stage.pop_32_batch()\n",
        "  datalist_1x32.extend(stage.pop_32_batch())\n",
        "  datalist_1x32.extend(stage.pop_32_batch())\n",
        "\n",
        "  # Batch: setup\n",
        "  completed_samples_1 = list()\n",
        "  completed_samples_2 = list()\n",
        "  completed_samples_3 = list()\n",
        "  completed_samples_4 = list()\n",
        "  completed_samples_5 = list()\n",
        "  map_completed_samples = [\n",
        "    completed_samples_1,\n",
        "    completed_samples_2,\n",
        "    completed_samples_3,\n",
        "    completed_samples_4,\n",
        "    completed_samples_5]\n",
        "\n",
        "\n",
        "  while len(stage.prompts) > 0:\n",
        "    prompt = stage.prompts.pop()\n",
        "    temp_completed_samples = list()\n",
        "    # Beam Search(1x96)\n",
        "    for sample in datalist_1x32:\n",
        "      #answer\n",
        "      answer_category, answer_reason = call_with_timeout(\n",
        "      answer,\n",
        "      timeout=5,\n",
        "      max_retries=100,\n",
        "      subject=sample['subject'],\n",
        "      body=sample['body'],\n",
        "      prompt=prompt)\n",
        "      #validation\n",
        "      sample['llm_answer'] = answer_category\n",
        "      sample['full_llm_answer'] = \"answer: \" + answer_category + \"\\nreason: \" + answer_reason\n",
        "      sample['prompt'] = prompt\n",
        "      sample['round'] = stage.round\n",
        "      sample['timestamp'] = datetime.now().isoformat()\n",
        "      sample[\"correctness\"] = (sample['queue'].lower() == sample['llm_answer'].lower())\n",
        "      temp_completed_samples.append(sample)\n",
        "    debug_counts += 96\n",
        "    if debug_counts % 100 == 0:\n",
        "      logging.critical(f\"Processed {debug_counts} samples\")\n",
        "    # === Append to a csv file ===\n",
        "    # Assign result to the first empty one\n",
        "    for sample_list in map_completed_samples:\n",
        "      if not sample_list:\n",
        "          sample_list.extend(temp_completed_samples)\n",
        "          break\n",
        "    append_samples_csv(temp_completed_samples, output_beamsearch_filepath)\n",
        "  # === 5 prompts beam search done & Update Round===\n",
        "  stage.round += 1\n",
        "  prompts_scores = list()\n",
        "  best_samples = list()\n",
        "  best_score = 0\n",
        "  best_prompt = \"\"\n",
        "  for samples in map_completed_samples:\n",
        "    if not samples:\n",
        "      continue\n",
        "    df_test_completed_samples = pd.DataFrame(samples)\n",
        "\n",
        "    f1_score = classification_report(df_test_completed_samples['queue'], df_test_completed_samples['llm_answer'], output_dict=True, zero_division=0)[\"macro avg\"][\"f1-score\"]\n",
        "    best_prompt = samples[0]['prompt']\n",
        "    prompts_scores.append((best_prompt, f1_score))\n",
        "    if f1_score > best_score:\n",
        "      best_score = f1_score\n",
        "      best_samples = samples\n",
        "  top_1_prompts_scores = [(prompt, score) for prompt, score in sorted(prompts_scores, key=lambda x: x[1], reverse=True)[:1]]\n",
        "  stage.prompts = [top_1_prompts_scores[0][0]]\n",
        "  logging.critical(f\"Best Score: {str(top_1_prompts_scores[0][1])}\")\n",
        "  # === Update Prompt ===\n",
        "  trajectories = answers2trajectories(best_samples)\n",
        "  better_prompts = generate_multiple_prompts(trajectories, 4)\n",
        "  stage.prompts.extend(better_prompts)\n",
        "  stage.prompts.append(best_prompt)\n",
        "\n",
        "logging.critical(\"ALL SAMPLES PROCESSED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0G_6AhnrSac"
      },
      "outputs": [],
      "source": [
        "email_classification_prompt = \"\"\"Classify the following customer emails into one of these categories:\n",
        "\n",
        "- Technical Support\n",
        "- Product Support\n",
        "- Customer Service\n",
        "- IT Support\n",
        "- Billing and Payments\n",
        "\n",
        "### Examples:\n",
        "\n",
        "Email 1:\n",
        "\"My app keeps crashing whenever I try to upload a file. Can someone help me fix this?\"\n",
        "Category: Technical Support\n",
        "\n",
        "Email 2:\n",
        "\"I’m trying to understand how to use the new dashboard feature. Is there a guide?\"\n",
        "Category: Product Support\n",
        "\n",
        "Email 3:\n",
        "\"I was overcharged on my last invoice and need a refund.\"\n",
        "Category: Billing and Payments\n",
        "\n",
        "Email 4:\n",
        "\"I can’t log in to the company VPN, and I have an urgent meeting.\"\n",
        "Category: IT Support\n",
        "\n",
        "Email 5:\n",
        "\"I had a great experience with your team and just wanted to say thank you!\"\n",
        "Category: Customer Service\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "puHsLeQGu01M"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}